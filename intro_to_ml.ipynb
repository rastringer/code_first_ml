{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/code_first_ml/blob/main/intro_to_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsy0-MYTResC"
      },
      "source": [
        "## Introduction to Machine Learning\n",
        "\n",
        "Machine learning (ML) is a field within Artificial Intelligence (AI) which has been around for decades. Various advances in computational power (accelerators such as GPUs) and algorithm design have led to stunning advances in the last decade.\n",
        "\n",
        "In this notebook, we will examine typical approaches to ML; some of the mathematics involved, what 'training' means in model development, and common considerations for 'MLOps', or ML in production.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchviz torch torchvision shap"
      ],
      "metadata": {
        "id": "93DJ83WjTQl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQF2tNP2SXpu"
      },
      "source": [
        "<img src=\"https://blog.hnf.de/wp-content/uploads/2020/12/Arthur_Samuel.jpg\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF0UkETiTrzw"
      },
      "source": [
        "#### Early days\n",
        "\n",
        "Arthur Samuel, IBM researcher in 1962:\n",
        "\n",
        "“Programming a computer...is, at best, a difficult task, not primarily because of any inherent complexity in the computer itself but, rather, because of the need to spell out every minute step of the process in the most exasperating detail. Computers, as any programmer will tell you, are giant morons, not giant brains.”\n",
        "\n",
        "In traditional computing, we take inputs, perform some prescribed operations, and generate an output.\n",
        "\n",
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/input_program_output.png?raw=true\" width=\"600\"/>\n",
        "\n",
        "\n",
        "```\n",
        "def square(x):\n",
        "   return x*x\n",
        "```\n",
        "\n",
        "Samuel referred to the idea of assigning weights to inputs, which could then be adjusted to maximize the performance of a particular task.\n",
        "\n",
        "Samuel:\n",
        "\n",
        "“Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. ”\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/input_model_weights.png?raw=true\" width=\"500\"/>\n",
        "\n",
        "\"We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would \"learn\" from its experience.” –Samuel.\n",
        "\n",
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/input_model_backprop.png?raw=true\" width=\"600\"/>\n",
        "\n",
        "\n",
        "A sporting analogy is that of an athlete practicing their discipline, with a good coach who ensures they are learning to improve their skills as they perform them repeatedly.\n",
        "\n",
        "<img src=\"https://d1s9j44aio5gjs.cloudfront.net/2020/07/Becoming_a_swimming_coach_Careers_in_Aquatics.jpg\" width=\"600\"/>\n",
        "\n",
        "We will explore each of these themes in more detail as we progress through the notebook, however here, in a nutshell, outlined in the 1960s, are the basics of what became the burgeoning field of machine learning.\n",
        "\n",
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/flowers_example.png?raw=true\" width=\"600\"/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFAuV1FWhKh6"
      },
      "source": [
        "### Principles and code\n",
        "\n",
        "Let's explore the building blocks of a neural network. Grateful for Jeremy Howard's teaching methodology and [related notebook](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work) for the following materials.\n",
        "If today interests you, check out his courses at [fast.ai](https://www.fast.ai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOkY6ZGtixhn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "\n",
        "def plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n",
        "    x = torch.linspace(min,max, 100)[:,None]\n",
        "    if ylim: plt.ylim(ylim)\n",
        "    plt.plot(x, f(x), color)\n",
        "    if title is not None: plt.title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMt7NR1vIK4u"
      },
      "source": [
        "### Quadratic functions\n",
        "Quadratic functions can be useful for modeling the trajectory of projectiles, arcs and parabolic shapes. They are often used in optimization problems, or to fit real-world data.\n",
        "\n",
        "For this notebook, we have no use in mind other than to demonstrate some of the building blocks of a neural network.\n",
        "\n",
        "This function shows $ax^2+bx+c$, with parameters $a = 1, b = 2, c = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GED65qRCIVoX"
      },
      "outputs": [],
      "source": [
        "def quadratic(a, b, c, x):\n",
        "  return a*x**2 + b*x + c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMqkNkCzlYMX"
      },
      "outputs": [],
      "source": [
        "# Setup quadratic\n",
        "f = lambda x: quadratic(1, -2, 1, x)\n",
        "\n",
        "# Generate x and y values over a range\n",
        "x = np.linspace(-2.1, 2.1, 100)[:,None]\n",
        "y = f(x)\n",
        "\n",
        "# Create plot\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x, y, '-r', label='Quadratic function')\n",
        "ax.legend()\n",
        "ax.set_title('Simple Quadratic Function')\n",
        "ax.set_xlabel('X value')\n",
        "ax.set_ylabel('Y = f(x)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR9eqPufjmz3"
      },
      "source": [
        "\n",
        "\n",
        "We can fix these values using a `partial` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKS04KZYiZz0"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "def make_quad(a,b,c):\n",
        "  return partial(quadratic, a,b,c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evp99B2BkLnf"
      },
      "outputs": [],
      "source": [
        "f2 = make_quad(3,2,1)\n",
        "plot_function(f2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM9xuCbXngkW"
      },
      "source": [
        "Add random noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHp1Fy5rkh4h"
      },
      "outputs": [],
      "source": [
        "def noise(x, scale):\n",
        "  return np.random.normal(scale=scale, size=x.shape)\n",
        "\n",
        "def add_noise(x, mult, add):\n",
        "  return x * (1+noise(x,mult)) + noise(x,add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmR5CAy_nj2l"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "x = torch.linspace(-2, 2, steps=20)[:,None]\n",
        "y = add_noise(f(x), 0.15, 1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA7emqQrnn6d"
      },
      "outputs": [],
      "source": [
        "x[:5],y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX27VDPAJKPT"
      },
      "source": [
        "What are these tensors?\n",
        "\n",
        "Tensors are an array of numerical values, just like scalars, vectors and matricies. They can have any number of dimensions. For example, an image is a 3D tensor with height, width and depth dimensions.\n",
        "Dimensions are also commonly referred to as 'rank'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bdJM0YPJimi"
      },
      "outputs": [],
      "source": [
        "tensor = torch.rand(3, 4)\n",
        "print(f\"Tensor = {tensor}\")\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCMV2R9cJiuT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Scalar\n",
        "scalar = torch.tensor(5)\n",
        "print(f\"Scalar rank: {scalar.dim()}\")\n",
        "\n",
        "# Vector\n",
        "vec = torch.tensor([1, 2, 3])\n",
        "print(f\"Vector rank: {vec.dim()}\")\n",
        "\n",
        "# Matrix\n",
        "mat = torch.tensor([[1, 2], [3, 4]])\n",
        "print(f\"Matrix rank: {mat.dim()}\")\n",
        "\n",
        "# 3D Tensor\n",
        "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "print(f\"3D tensor rank: {tensor3d.dim()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeqs5c5DKAuT"
      },
      "source": [
        "Back to the quadratic. Let's add some noise to explore fitting our quadratic to points in a graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrqFcb5inp5e"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x,y);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_-opIEdnsR3"
      },
      "outputs": [],
      "source": [
        "@interact(a=1.1, b=1.1, c=1.1)\n",
        "def plot_quad(a, b, c):\n",
        "    plt.scatter(x,y)\n",
        "    plot_function(make_quad(a,b,c), ylim=(-3,13))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRkLhw0kLrVk"
      },
      "source": [
        "Our experimentation would be a lot more efficient if we had a way of calculating our error to help us move towards improvement faster. Introducing...\n",
        "\n",
        "### Mean Absolute Error\n",
        "\n",
        "MAE one possible 'loss function' for machine learning. It measures errors between a prediction and actual. MAE is calculated as the sum of absolute errors divided by the sample size:\n",
        "\n",
        "\n",
        "$\\mathit{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_{true}^{(i)} - y_{pred}^{(i)}|$\n",
        "\n",
        "As is often the case in ML, this looks far simpler in code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB7hy6wgnxoO"
      },
      "outputs": [],
      "source": [
        "def mae(preds, acts):\n",
        "  return (torch.abs(preds-acts)).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K40rj3SILlNG"
      },
      "source": [
        "A useful feature of Colab or Jupyter notebooks is using `??` to check the documentation for a particular method. Let's look at `.abs`, part of the PyTorch library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kbBnM_tLb4w"
      },
      "outputs": [],
      "source": [
        "torch.abs??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J3S6_g6n8Kv"
      },
      "outputs": [],
      "source": [
        "@interact(a=1.1, b=1.1, c=1.1)\n",
        "def plot_quad(a, b, c):\n",
        "    f = make_quad(a,b,c)\n",
        "    plt.scatter(x,y)\n",
        "    loss = mae(f(x), y)\n",
        "    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDfG8krgMoqb"
      },
      "source": [
        "### Gradient descent\n",
        "\n",
        "If we know the gradient of our MAE function with respect to `a`, `b`, and `c`, then we can workout how adjusting either one of the parameters may change the value of the MAE function.\n",
        "\n",
        "For example, is `a` has a negative gradient, increasing the value of `a` will decrease our MAE loss, which of course we want to make as low as possible.\n",
        "\n",
        "We therefore need a function that takes `a`, `b`, and `c` as a vector input, and returns the MAE based on those parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-jmVoKHn_AV"
      },
      "outputs": [],
      "source": [
        "def quad_mae(params):\n",
        "    f = make_quad(*params)\n",
        "    return mae(f(x), y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aPcRvIgNeya"
      },
      "source": [
        "The result should be the same result MAE gave us above for the first plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjJmzoVloHCP"
      },
      "outputs": [],
      "source": [
        "quad_mae([1.1, 1.1, 1.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRMUFugxNiKY"
      },
      "source": [
        "Let's try it on basic initialized values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv5klSHcoIYF"
      },
      "outputs": [],
      "source": [
        "abc = torch.tensor([1.1,1.1,1.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ksVPrqNusf"
      },
      "source": [
        "To calculate the gradients, we just use `requires_grad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFthvlcXoMt4"
      },
      "outputs": [],
      "source": [
        "abc.requires_grad_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfkW-6GroOF_"
      },
      "outputs": [],
      "source": [
        "loss = quad_mae(abc)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmaNwyQ_gHDi"
      },
      "source": [
        "Backward pass:\n",
        "\n",
        "Output layer's gradient:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\text{loss}}{\\partial Y} = 2(Y - \\hat{y})\n",
        "\\end{equation}\n",
        "\n",
        "Propagate the gradient backwards:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\text{loss}}{\\partial W_l} &= \\frac{\\partial \\text{loss}}{\\partial Y} \\odot g'(W_l \\cdot Y) \\odot Y \\\n",
        "\\frac{\\partial \\text{loss}}{\\partial Y_{l-1}} &= \\frac{\\partial \\text{loss}}{\\partial Y} \\odot g'(W_l \\cdot Y) \\cdot W_l^T\n",
        "\\end{align*}\n",
        "\n",
        "where:\n",
        "\n",
        "⊙ denotes the element-wise dot product.\n",
        "g\n",
        "′\n",
        "  is the derivative of the activation function g.\n",
        "Update the weights:\n",
        "\\begin{equation}\n",
        "W_l \\leftarrow W_l - \\eta \\cdot \\frac{\\partial \\text{loss}}{\\partial W_l}\n",
        "\\end{equation}\n",
        "\n",
        "where η is the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT8aYoPXfCLx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Let's say your network has L layers,\n",
        "where each layer has an activation function g and a weight matrix W.\n",
        "The input to the network is X, and the output is Y.\n",
        "\"\"\"\n",
        "\n",
        "Y = X\n",
        "for l in range(1, L+1):\n",
        "  Y = g(W_l @ Y)\n",
        "\n",
        "\"\"\"\n",
        "Calculate the loss\n",
        "Define the loss function eg Mean Squared Error to minimize\n",
        "\"\"\"\n",
        "\n",
        "loss = (Y - y_hat)^2\n",
        "\n",
        "\"\"\"\n",
        "Backward pass:\n",
        "Calculate the output layer's gradient\n",
        "\"\"\"\n",
        "\n",
        "dL/dY = 2 * (Y - y_hat)\n",
        "\n",
        "\"\"\"\n",
        "Propagate the gradient backwards:\n",
        "For each layer l in reverse order (L, L-1, ..., 1):\n",
        "\"\"\"\n",
        "\n",
        "dW_l = dL/dY * g'(W_l @ Y) * Y\n",
        "dY = dL/dY * g'(W_l @ Y) * W_l.T\n",
        "\n",
        "\"\"\"\n",
        "dW_l is the gradient of the loss with respect to the weights of layer l.\n",
        "dY is the gradient of the loss with respect to the output of layer l-1.\n",
        "g'(z) is the derivative of the activation function g evaluated at z.\n",
        "\"\"\"\n",
        "\n",
        "# Update the weights\n",
        "W_l = W_l - learning_rate * dW_l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duGQnX98OFZV"
      },
      "source": [
        "In reality, ML frameworks make this a lot easier.\n",
        "\n",
        "Calculating the gradients from our `quad_mae` loss above in PyTorch is straightforward, using `backward()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da7hbjPOoP6O"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMKH87juOXnt"
      },
      "source": [
        "The gradients are accessible via `.grad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-pnwKSAoRZm"
      },
      "outputs": [],
      "source": [
        "abc.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9TaqKNvoTJw"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    abc -= abc.grad*0.01\n",
        "    loss = quad_mae(abc)\n",
        "\n",
        "print(f'loss={loss:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2GnYHpHoVog"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    loss = quad_mae(abc)\n",
        "    loss.backward()\n",
        "    with torch.no_grad(): abc -= abc.grad*0.01\n",
        "    print(f'step={i}; loss={loss:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7nBb0ltgSTC"
      },
      "source": [
        "Here are all the steps together\n",
        "\n",
        "**Forward pass**:\n",
        "\n",
        "**Define the network**:\n",
        "\\begin{align*}\n",
        "Y &= X \\\n",
        "\\text{for } l &= 1, \\dots, L: \\\n",
        "Y &= g(W_l \\cdot Y)\n",
        "\\end{align*}\n",
        "\n",
        "**Calculate the loss**:\n",
        "\\begin{equation}\n",
        "\\text{loss} = \\frac{1}{2} || Y - \\hat{y} ||^2\n",
        "\\end{equation}\n",
        "\n",
        "**Backward pass**:\n",
        "\n",
        "**Output layer's gradient**:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\text{loss}}{\\partial Y} = 2(Y - \\hat{y})\n",
        "\\end{equation}\n",
        "\n",
        "**Propagate the gradient backwards**:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\text{loss}}{\\partial W_l} &= \\frac{\\partial \\text{loss}}{\\partial Y} \\odot g'(W_l \\cdot Y) \\odot Y \\\n",
        "\\frac{\\partial \\text{loss}}{\\partial Y_{l-1}} &= \\frac{\\partial \\text{loss}}{\\partial Y} \\odot g'(W_l \\cdot Y) \\cdot W_l^T\n",
        "\\end{align*}\n",
        "\n",
        "**where**:\n",
        "\n",
        "⊙ denotes the element-wise dot product.\n",
        "g\n",
        "′\n",
        "  is the derivative of the activation function g.\n",
        "Update the weights:\n",
        "\\begin{equation}\n",
        "W_l \\leftarrow W_l - \\eta \\cdot \\frac{\\partial \\text{loss}}{\\partial W_l}\n",
        "\\end{equation}\n",
        "\n",
        "where η is the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y6z6cKtz4pg"
      },
      "source": [
        "### From quadratic functions to approximating to any computable function.\n",
        "\n",
        "The two core building blocks that allow neural networks to classify images, generate text, and translate languages are matrix multiplication and activation functions.\n",
        "\n",
        "### Matrix multiplication\n",
        "\n",
        "AKA matmul, computes the weighted sum of multiple inputs to produce outputs at each neuron.\n",
        "\n",
        "The weighted matrix, which contains connection weights between neurons in adjacent layers, and the input vector, containing the input values fed into the current layer of neurons, are multiplied together to create an output vector.\n",
        "\n",
        "See [matrixmultiplication.xyz](matrixmultiplication.xyz) for a helpful visualization of this operation.\n",
        "\n",
        "### Activation function\n",
        "\n",
        "Activations introduce non-linearity, which allows neural networks to learn complex patterns in data. Without non-linear activations, a neural network is essentially just a linear regression model.\n",
        "\n",
        "They also constrain the range of outputs, by squashing inputs to a range like 0-1 or -1 to 1. This bounds neuron outputs which has benefits for model training.\n",
        "\n",
        "Here is one of the most commonly used activation functions, the Rectified Linear Unit. This function simply replaces all negative numbers with zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFRDHuO-oXv4"
      },
      "outputs": [],
      "source": [
        "def rectified_linear(m,b,x):\n",
        "    y = m*x+b\n",
        "    return torch.clip(y, 0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKVq7r6O1hf1"
      },
      "outputs": [],
      "source": [
        "plot_function(partial(rectified_linear, 1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kyp4Oc9v1jOk"
      },
      "outputs": [],
      "source": [
        "@interact(m=1.5, b=1.5)\n",
        "def plot_relu(m, b):\n",
        "    plot_function(partial(rectified_linear, m,b), ylim=(-1,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alhJ8B8O10oM"
      },
      "outputs": [],
      "source": [
        "def double_relu(m1,b1,m2,b2,x):\n",
        "    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n",
        "\n",
        "@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\n",
        "def plot_double_relu(m1, b1, m2, b2):\n",
        "    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xFPXm3oagT2"
      },
      "source": [
        "While there are many pre-processing techniques depending on whether a model is to be trained on images, or text etc, and many different model architectures, most boil down to inventive and efficient combinations of matrix multiplications and activation functions.\n",
        "\n",
        "### A beginner neural network\n",
        "\n",
        "Let's look at a simple neural net written from scratch. Again, major thanks to Jeremy Howard for this [example](https://pytorch.org/tutorials/beginner/nn_tutorial.html) on PyTorch.org.\n",
        "\n",
        "Models need data, and we will use the classic MNIST dataset, which comprises black and white images of hand-drawn digits between 0-9. This is a very common starting point in ML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyzPmZKv16_z"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists():\n",
        "        content = requests.get(URL + FILENAME).content\n",
        "        (PATH / FILENAME).open(\"wb\").write(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4qSfmNvbQR3"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhRSgO1CgW8m"
      },
      "source": [
        "The images are stored as a flattened row, 784 in length. To display an image, we can reshape it to 2d.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr4_Gh2DbRtj"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "\n",
        "pyplot.imshow(x_train[18].reshape((28, 28)), cmap=\"gray\")\n",
        "# ``pyplot.show()`` only if not on Colab\n",
        "try:\n",
        "    import google.colab\n",
        "except ImportError:\n",
        "    pyplot.show()\n",
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL-IgKvggch2"
      },
      "outputs": [],
      "source": [
        "type(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Hq6m3fgfBO"
      },
      "source": [
        "Since the image type is numpy array, we convert to PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwpu7OEzbS1q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = map(\n",
        "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
        ")\n",
        "n, c = x_train.shape\n",
        "print(x_train, y_train)\n",
        "print(x_train.shape)\n",
        "print(y_train.min(), y_train.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyCWGhrXgpqX"
      },
      "source": [
        "### Neural network from tensor operations\n",
        "\n",
        "We create weight tensors using *Xavier initialisation*, which means we multiply $1/sqrt(n)$. There are many approaches to initializing weights, generally we want to assign small values to begin with, that we can update throughout the training cycle.\n",
        "\n",
        "Specifying `requires_grad()` here means the tensor weights require a gradient. PyTorch will store the operations carried out on the tensor, enabling automatic gradient calculation during backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYBL_KFjbUuj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "weights = torch.randn(784, 10) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGRZJH4PjOsG"
      },
      "source": [
        "### Activation function\n",
        "\n",
        "We use `log_softmax`, which uses the log probabilities to convert prediction scores for each class into probabilities that sum to 1. (eg probability this is a '3': 0.6; a '4': 0.3, an '8': 0.1).\n",
        "\n",
        "This typically would only be done for the output layer of the neural network, however since we have a network of one 'layer' (essentially a function in this case), it suffices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMM4w2Xhbdpb"
      },
      "outputs": [],
      "source": [
        "def log_softmax(x):\n",
        "    return x - x.exp().sum(-1).log().unsqueeze(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9rZwBUlkC79"
      },
      "source": [
        "### Model\n",
        "\n",
        "Our model is somewhat simpler than most current ML architectures. We use matrix multiplication and the `log_softmax` activation function.\n",
        "\n",
        "`@` is shorthand for the multiplication eg `multiply xb (a batch of training examples) by weights`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BCL_tqDkCWN"
      },
      "outputs": [],
      "source": [
        "def model(xb):\n",
        "    return log_softmax(xb @ weights + bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXEIHumlAqg"
      },
      "source": [
        "### Feed forward neural net\n",
        "\n",
        "We now have a feed forward neural network. The results will not be accurate since we start with random weights and have just one optimization step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEreFDYLbe70"
      },
      "outputs": [],
      "source": [
        "bs = 64  # batch size\n",
        "xb = x_train[0:bs]  # a mini-batch from x\n",
        "preds = model(xb)  # predictions\n",
        "preds[0], preds.shape\n",
        "print(preds[0], preds.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAhEk0ColRGG"
      },
      "source": [
        "At this stage, we have a matrix multiplication to multiply inputs by weights. We add a bias. Then an activation squashes the values to scores in the range 0-1 for each class.\n",
        "\n",
        "We just need a loss function to work out how wrong our predictions are.\n",
        "\n",
        "Here's negative loss likelihood, another approach to loss functions.\n",
        "\n",
        "$\\mathit{L}(\\hat{y}, y) = -\\log(\\hat{y}_y)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZf_fEORbf61"
      },
      "outputs": [],
      "source": [
        "def neg_loss_likelihood(input, target):\n",
        "    return -input[range(target.shape[0]), target].mean()\n",
        "\n",
        "loss_func = neg_loss_likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WD_BfeQbiQ0"
      },
      "outputs": [],
      "source": [
        "yb = y_train[0:bs]\n",
        "print(loss_func(preds, yb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxkFEfXqbjS7"
      },
      "outputs": [],
      "source": [
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwx0Xh7eblL0"
      },
      "outputs": [],
      "source": [
        "print(accuracy(preds, yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5iex_jwgn8Q"
      },
      "source": [
        "We can now run a training loop. For each iteration, we will:\n",
        "\n",
        "select a mini-batch of data (of size bs)\n",
        "\n",
        "use the model to make predictions\n",
        "\n",
        "calculate the loss\n",
        "\n",
        "loss.backward() updates the gradients of the model, in this case, weights and bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "royyFZ-kbm6r"
      },
      "outputs": [],
      "source": [
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "lr = 0.02  # learning rate\n",
        "epochs = 4  # how many epochs to train for\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bs + 1):\n",
        "        #         set_trace()\n",
        "        start_i = i * bs\n",
        "        end_i = start_i + bs\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            weights -= weights.grad * lr\n",
        "            bias -= bias.grad * lr\n",
        "            weights.grad.zero_()\n",
        "            bias.grad.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXoA3EGRc69S"
      },
      "outputs": [],
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cXP9XOHc-Vd"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "def model(xb):\n",
        "    return xb @ weights + bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-x0db3edXr0"
      },
      "outputs": [],
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaQkf0VU4CD2"
      },
      "source": [
        "### Now for the easy way\n",
        "\n",
        "Let's use the features of PyTorch to train an MNIST model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19CAy4U_gyEF"
      },
      "source": [
        "### Hyperparameters\n",
        "\n",
        "Hyperparameters govern some of the behaviour of a neural network. Tuning hyperparameters is a key concern for a machine learning workload. Typically engineers will run several experiments to find the best combination of hyperparameters on subsets of data before full (and more expensive) training runs.\n",
        "\n",
        "Typically in model training notebooks you would see hyperparmas appear like environment vars:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RhYKAaXUHoa"
      },
      "outputs": [],
      "source": [
        "# EPOCHS = 5 # How many times the entire training dataset is passed through the net\n",
        "\n",
        "# BATCH_SIZE = 64 # No. of data samples processed at once during training\n",
        "# LR = 0.001 # Small value used in weight updates based on calculated gradients\n",
        "# SEED = 1 # Random seed for network initialization. Ensures reproducible results\n",
        "# LOG_INTERVAL = 100 # Interval at which training results are logged"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AgQII_Xqar_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "V4y6pJnjau2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access and view elements from the DataLoader\n",
        "for batch in train_loader:\n",
        "    images, labels = batch\n",
        "    # You can now work with the batch of images and labels\n",
        "    # For example, printing the shape of the batch\n",
        "    print(\"Batch of images shape:\", images.shape)\n",
        "    print(\"Batch of labels shape:\", labels.shape)\n",
        "    image_tensor = images[:5]\n",
        "    print(\"Image tensor shape:\", image_tensor.shape)\n",
        "    break  # Stop after processing the first batch"
      ],
      "metadata": {
        "id": "qxtNmkjwazE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ],
      "metadata": {
        "id": "bIq5cfFSbBOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig"
      ],
      "metadata": {
        "id": "SCntV-Sia707"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleNN().to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model and profile with PyTorch Profiler\n",
        "with profile(activities=[ProfilerActivity.CUDA], record_shapes=True, use_cuda=True) as prof:\n",
        "    for epoch in range(5):\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            with record_function(\"model_inference\"):\n",
        "              inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        "              outputs = model(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Print results at the end of each epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch + 1}/{5}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "torch.save(model.state_dict(), 'mnist_model.pth')\n",
        "# Save profiler results\n",
        "prof.export_chrome_trace(\"profile_results.json\")\n"
      ],
      "metadata": {
        "id": "IGoYa6ViVZwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the model"
      ],
      "metadata": {
        "id": "F-sVs4w7TbBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, (1, 28, 28))  # Assuming input image size is (1, 28, 28)"
      ],
      "metadata": {
        "id": "tsVvMY49TK3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Better still, we can use the `torchviz` library to visualize the various building blocks of the network."
      ],
      "metadata": {
        "id": "XrMKSnyhTfwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "# Same size as input data\n",
        "dummy_input = torch.randn(1, 1, 28, 28).cuda()\n",
        "\n",
        "graph = make_dot(model(dummy_input), params=dict(model.named_parameters()))\n",
        "graph.render(\"CNNModel\", format=\"png\", cleanup=True)"
      ],
      "metadata": {
        "id": "GxQgslV3TdmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Display the image in the notebook\n",
        "image_path = \"CNNModel.png\"\n",
        "display(Image(filename=image_path))"
      ],
      "metadata": {
        "id": "EUDKcFcYTsSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8aCKreD8c9T"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"mnist_model.pth\")\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   output = model(example_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpreting the model"
      ],
      "metadata": {
        "id": "lz-7Z8w4UWin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the test dataset and DataLoader\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load the trained model\n",
        "model = SimpleNN()\n",
        "model.load_state_dict(torch.load('mnist_model.pth'))  # Replace with the actual path to your trained model file\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Use the model to make predictions on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with profile(activities=[\n",
        "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "        with torch.no_grad():\n",
        "          for inputs, labels in test_loader:\n",
        "            with record_function(\"model_inference\"):\n",
        "              outputs = model(inputs)\n",
        "              _, predicted = outputs.max(1)\n",
        "              total += labels.size(0)\n",
        "              correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "FGWUk8iRVCoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Select a few samples from the MNIST dataset for interpretation\n",
        "batch_size = 128\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# since shuffle=True, this is a random sample of test data\n",
        "batch = next(iter(test_loader))\n",
        "images, _ = batch\n",
        "\n",
        "background = images[:100]\n",
        "test_images = images[100:103]\n",
        "\n",
        "e = shap.DeepExplainer(model, background)\n",
        "shap_values = e.shap_values(test_images)"
      ],
      "metadata": {
        "id": "8z4sktrVUTRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n"
      ],
      "metadata": {
        "id": "K5Lufh7OZ4kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n",
        "test_numpy = np.swapaxes(np.swapaxes(test_images.numpy(), 1, -1), 1, 2)\n",
        "\n",
        "# plot the feature attributions\n",
        "shap.image_plot(shap_numpy, -test_numpy)"
      ],
      "metadata": {
        "id": "WujYLN1XaFdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matmul and accelerators"
      ],
      "metadata": {
        "id": "5LojEZgHaCF5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMsQxm7EaWWY"
      },
      "outputs": [],
      "source": [
        "image_tensor = x_train[100:110]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY2cnn8PoVwf"
      },
      "outputs": [],
      "source": [
        "image_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlbdjB_6oa7w"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "weights = torch.randn(784, 10)\n",
        "bias = torch.zeros(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjyx8jMRofKX"
      },
      "outputs": [],
      "source": [
        "a = image_tensor\n",
        "b = weights\n",
        "a.shape, b.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITr2L4SOogzP"
      },
      "outputs": [],
      "source": [
        "# a rows, a columns\n",
        "ar, ac = a.shape\n",
        "# b rows, b columns\n",
        "br, bc = b.shape\n",
        "\n",
        "(ar, ac), (br, bc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpHUCK4Hol-5"
      },
      "outputs": [],
      "source": [
        "t1 = torch.zeros(ar, bc)\n",
        "t1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EIoWX-Xoomo"
      },
      "outputs": [],
      "source": [
        "t1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LogK852Bp_aD"
      },
      "outputs": [],
      "source": [
        "# Check for CUDA\n",
        "device = \"cuda\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# # Set seed for reproducibility\n",
        "# torch.manual_seed(SEED)\n",
        "\n",
        "# if device == \"cuda\":\n",
        "#     torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# print(f\"Performing computations on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matmul on CPU"
      ],
      "metadata": {
        "id": "1WIjKUTTfkbt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8gXFNHdopwg"
      },
      "outputs": [],
      "source": [
        "def matmul_simple(a, b):\n",
        "  (ar,ac),(br,bc) = a.shape,b.shape\n",
        "  t1 = torch.zeros(ar, bc)\n",
        "  for i in range(ar):\n",
        "    for j in range(bc):\n",
        "      for k in range(ac):\n",
        "        t1[i][j] += a[i][k] * b[k][j]\n",
        "\n",
        "  return t1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA-Y98mmor5x"
      },
      "outputs": [],
      "source": [
        "%timeit matmul_simple(a, b) # around 1.78s on CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numba / CUDA"
      ],
      "metadata": {
        "id": "eByLVZhZfnXO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu58p94_otjx"
      },
      "outputs": [],
      "source": [
        "import numba as nb\n",
        "from numba import njit\n",
        "import numpy as np\n",
        "\n",
        "a_np = a.numpy()\n",
        "b_np = b.numpy()\n",
        "\n",
        "@nb.jit(nopython=True)\n",
        "def matmul_numba(a, b):\n",
        "  ar,ac = a_np.shape\n",
        "  br,bc = b_np.shape\n",
        "  t1 = np.zeros((ar, bc))\n",
        "  for i in range(ar):\n",
        "    for j in range(bc):\n",
        "      dot_product = 0.0\n",
        "      for k in range(ac):\n",
        "        dot_product += a[i][k] * b[k][j]\n",
        "      t1[i][j] = dot_product\n",
        "  return t1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEcnoztzo675"
      },
      "outputs": [],
      "source": [
        "%timeit matmul_numba(a_np, b_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkS16HmMqjJW"
      },
      "outputs": [],
      "source": [
        "# 50,000 matmul at 114 microseconds\n",
        "0.000114 * 50000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 50,000 matmul at 1.57 seconds (in minutes)\n",
        "1.57 * 50000 / 60"
      ],
      "metadata": {
        "id": "r8q6DR6eeDIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jonf3Nt0fckb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQATH2GqJ4mVvBqOBoO1cn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}