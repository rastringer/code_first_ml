{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPs5XI8pl5KJHO460WgbDr9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/code_first_ml/blob/main/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6QvASwZszbT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers"
      ],
      "metadata": {
        "id": "pnlZuh9ntYro"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pqZUWZevtbdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kPsIUJ8pwcYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/transformer_architecture.png?raw=true\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "KL_Dc3p9wchG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Tdpv6mTQaWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "vAsyciDvQHnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the pre-trained English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Word vectors are awesome!\"\n",
        "\n",
        "# Process the sentence using spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Access the word vectors for each token in the sentence\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.vector[:5]}...\")  # Displaying the first 5 components of the vector\n"
      ],
      "metadata": {
        "id": "1iaQWrJ6wh4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083da97a-35a2-4dbf-dd52-890a74b281eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: [-0.6131599 -1.3359097  0.5144141  0.582945  -0.5807983]...\n",
            "vectors: [-0.16492632  1.2883415  -0.17733231  0.5517031   0.6896729 ]...\n",
            "are: [ 0.08803613  0.7008368  -0.5576158  -0.5089512   0.14558357]...\n",
            "awesome: [ 1.9866399  0.9055047 -0.7493017 -0.5317862 -0.8689432]...\n",
            "!: [-0.9200196  -0.08114609  0.08567562 -1.8008881   0.09100179]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/word_vectors.png?raw=true\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "RQ1j_n66VN0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The difficulties of word embeddings\n",
        "\n",
        "Word vectors and embeddings are very useful however the abstract nature of language can cause problems when assigning numerical values to words. For example, in the following sentences, \"trainers\" has a different meaning based on the context.\n",
        "\n",
        "\"Mustafa loved running in his new trainers\"\n",
        "\n",
        "\"Svitlana said the gym had the best trainers around\"\n",
        "\n",
        "Linguists call these words with unrelated meanings  *homonyms*. Another term is *polysemy*, which means a word can mean the same thing but have a slightly different meaning. For example,\n",
        "\n",
        "\"Joan wrote a program to calculate eucledian distance\"\n",
        "\n",
        "\"The program featured Mozart's The Marriage of Figaro\"\n",
        "\n",
        "In short, we need a way of finding the meaning of words based on their relevance to other words in the text. Step forward, Attention.\n",
        "\n"
      ],
      "metadata": {
        "id": "iBhfeBcWjJWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention\n",
        "\n",
        "There are two steps in the transformer during which the model learns what words and text mean. This in ML parlance is updating the \"hidden state\" for inputs to the model.\n",
        "\n",
        "The first is the attention stage, the transformer compares each word to all the other words in a sequence, looking for context and shared significance.\n",
        "\n",
        "The second is the feed forward step, where the model tries to capture more complex patterns and relationships between words. These are accomplished by mathematical transformations.\n",
        "\n",
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/attention_diagram.png?raw=true\" width=\"800\"/>\n",
        "\n",
        "[Diagram](https://distill.pub/2016/augmented-rnns/) from Olah and Carter, 2016"
      ],
      "metadata": {
        "id": "vHlu_HywWOoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From text to genomics and vision"
      ],
      "metadata": {
        "id": "2CdY5WvC42gI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/vit_transformer.png?raw=true\" width=\"800\"/>\n",
        "\n",
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/vit_attention.png?raw=true\" width=\"400\"/>\n",
        "\n",
        "Images from [\"An image is worth 16 x 16 words\"](https://arxiv.org/pdf/2010.11929.pdf)"
      ],
      "metadata": {
        "id": "85ockmDa4k91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-attention\n",
        "\n",
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/self_attention.png?raw=true\" width=\"800\"/>"
      ],
      "metadata": {
        "id": "x593Ys3ZPktQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/multi_head.png?raw=true\" width=\"800\"/>"
      ],
      "metadata": {
        "id": "zPgv0WH4Pkv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/transformer_encoder.png?raw=true\" width=\"800\"/>"
      ],
      "metadata": {
        "id": "LvAuNBn8n9aV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With each layer, the model's understanding of the text improves. Here are the outputs up to layer 23 of GPP-2 when given the following prompt:\n",
        "\n",
        "\"Q: What is the capital of France?\"\n",
        "\"A: Paris\"\n",
        "\"Q: What is the capital of Poland?\"\n",
        "\"A: \"\n",
        "\n",
        "```\n",
        "0  ( [ The:,\n",
        " at and Act A\n",
        "1  A The ( [ Is59\n",
        " At and40\n",
        "2  A [ ( The At Is Act at59,\n",
        "3  A [ ( Act At Is The CH An at\n",
        "4  A [ At Q (Q The Are M An\n",
        "5  A M No At The payable Q Qu (Q\n",
        "6  No M A The C Die An H En Qu\n",
        "7  C A No The M n P N H An\n",
        "8  A The C P H No n Ass N T\n",
        "9  A C No nil The Ch P An H N\n",
        "10  A The G C N P No Me An Le\n",
        "11  A C N None P G The Pr Ce H\n",
        "12  Unknown None C G A N Bar The Ch P\n",
        "13  C P N G B A Unknown St None The\n",
        "14  St N G P Poland B C Pol A D\n",
        "15  Poland P St Pol Warsaw Polish N B G Germany\n",
        "16  Poland Warsaw Polish Poles Budapest Prague Pol Germany Berlin Moscow\n",
        "17  Poland Warsaw Polish Poles Budapest Prague � Pol Lithuania Moscow\n",
        "18  Poland Warsaw Polish Prague Budapest Poles Moscow � Berlin Kiev\n",
        "19  Warsaw Poland Polish Budapest Prague Moscow Berlin Kiev � Frankfurt\n",
        "20  Warsaw Poland Prague Budapest Polish Moscow Kiev Berlin Frankfurt Brussels\n",
        "21  Warsaw Poland Polish Prague Budapest � Kiev Sz Berlin Moscow\n",
        "22  Warsaw Poland Prague Budapest K W Kiev Sz Moscow Berlin\n",
        "23  Warsaw W K Br Po B L Z P Poland\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "ZA2eQ6XcpW-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j3gTAg2IPky9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.query_proj = nn.Linear(embed_dim, self.head_dim * num_heads)\n",
        "        self.key_proj = nn.Linear(embed_dim, self.head_dim * num_heads)\n",
        "        self.value_proj = nn.Linear(embed_dim, self.head_dim * num_heads)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.output_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "\n",
        "        # Calculate queries, keys, and values (all with multiple heads)\n",
        "        queries = self.query_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        keys = self.key_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        values = self.value_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose for efficient dot-product attention calculation\n",
        "        queries = queries.transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attention_weights = self.softmax(scores)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        output = torch.matmul(attention_weights, values)\n",
        "\n",
        "        # Concatenate heads and project back to original embedding dimension\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "        output = output.view(batch_size, seq_len, embed_dim)\n",
        "        output = self.output_proj(output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HepFpPikQelu",
        "outputId": "4059a13c-0a30-4181-c2e6-4afe7d37c6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = SelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention\n",
        "        x = self.self_attention(x) + x  # Residual connection\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Feed-forward\n",
        "        x = self.feed_forward(x) + x  # Residual connection\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jnqTCvjotwbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have preprocessed data in source_sentence_tokens and target_sentence_tokens\n",
        "\n",
        "model = TransformerEncoder(vocab_size, embed_dim, num_layers, num_heads, hidden_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ...\n",
        "    source_embeddings = model(source_sentence_tokens)\n",
        "\n",
        "    # Hypothetical: Next steps would be feeding these embeddings to a decoder\n",
        "    # to generate the target language translation and calculate loss using criterion.\n",
        "    # ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "4yechJ6htyzz",
        "outputId": "c902ba65-f2f0-430e-d219-4d5e75570067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vocab_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6fec9df28518>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming you have preprocessed data in source_sentence_tokens and target_sentence_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "input_tensor = torch.randn(2, 5, 256)  # (batch_size, sequence_length, embedding_dim)\n",
        "attention_layer = SelfAttention(embed_dim=256, num_heads=8)\n",
        "output = attention_layer(input_tensor)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "qIRDzoz4tvfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the Transformer model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, d_model=256, nhead=4, num_encoder_layers=3, num_decoder_layers=3):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, output_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src)\n",
        "        tgt = self.embedding(tgt)\n",
        "\n",
        "        output = self.transformer(src, tgt)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Generate some toy data\n",
        "input_vocab_size = 10\n",
        "output_vocab_size = 10\n",
        "seq_length = 5\n",
        "batch_size = 2\n",
        "\n",
        "# Random input and target sequences with the same batch size\n",
        "input_sequence = torch.randint(0, input_vocab_size, (batch_size, seq_length))\n",
        "target_sequence = torch.randint(0, output_vocab_size, (batch_size, seq_length))  # Make sure the batch size matches\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(input_vocab_size, output_vocab_size)\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    output = model(input_sequence, target_sequence)\n",
        "\n",
        "    # Reshape output and target for loss calculation\n",
        "    output = output.view(-1, output_vocab_size)\n",
        "    target_sequence = target_sequence.view(-1)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = criterion(output, target_sequence)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every few epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing the model\n",
        "test_input_sequence = torch.randint(0, input_vocab_size, (batch_size, seq_length))\n",
        "output = model(test_input_sequence, target_sequence)\n",
        "predicted_sequence = torch.argmax(output, dim=2)\n",
        "\n",
        "print(\"Input Sequence:\")\n",
        "print(test_input_sequence)\n",
        "print(\"Target Sequence:\")\n",
        "print(target_sequence.view(batch_size, seq_length))\n",
        "print(\"Predicted Sequence:\")\n",
        "print(predicted_sequence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "T1aOWMtytE6n",
        "outputId": "97b02a3d-f17b-41c3-ff21-102a44b2aeaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "the batch number of src and tgt must be equal",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8ba652f393bc>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Reshape output and target for loss calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8ba652f393bc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mis_batched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the batch number of src and tgt must be equal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the batch number of src and tgt must be equal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: the batch number of src and tgt must be equal"
          ]
        }
      ]
    }
  ]
}