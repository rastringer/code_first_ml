{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqHJfCytidTEL2rok19P+8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/code_first_ml/blob/main/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6QvASwZszbT"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers"
      ],
      "metadata": {
        "id": "pnlZuh9ntYro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/transformer_architecture.png?raw=true\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "KL_Dc3p9wchG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Tdpv6mTQaWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word vectors\n",
        "\n",
        "### Tokenizers\n",
        "\n",
        "Since computers don't understand words, we use tokenizers to convert words into numbers. There are different ways to achieve this, including word-based, character-based and subword tokenization.\n",
        "\n",
        "\n",
        "\n",
        "One of the best and most accessible libraries for tokenization is from [Hugging Face](huggingface.co).\n",
        "\n",
        "To run the following cells, you will need a free account."
      ],
      "metadata": {
        "id": "vAsyciDvQHnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding"
      ],
      "metadata": {
        "id": "6pZqf5H9E8Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
        "\n",
        "sequence = \"Learning transformers is easy\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(f\"Tokens = {tokens}\\n\")\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f\"ids={ids}\")\n"
      ],
      "metadata": {
        "id": "2yBmLdhxDX2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Decoding\n",
        "\n",
        "decoded_string = tokenizer.decode([9681, 11303, 1468, 1110, 3123])\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "id": "dnvOqyLgE-Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the pre-trained English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Word vectors are awesome!\"\n",
        "\n",
        "# Process the sentence using spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Access the word vectors for each token in the sentence\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.vector[:5]}...\")  # Displaying the first 5 components of the vector\n"
      ],
      "metadata": {
        "id": "1iaQWrJ6wh4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/word_vectors.png?raw=true\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "RQ1j_n66VN0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The difficulties of word embeddings\n",
        "\n",
        "Word vectors and embeddings are very useful however the abstract nature of language can cause problems when assigning numerical values to words. For example, in the following sentences, \"trainers\" has a different meaning based on the context.\n",
        "\n",
        "*\"Mustafa loved running in his new trainers\"*\n",
        "\n",
        "*\"Svitlana said the gym had the best trainers around\"*\n",
        "\n",
        "Linguists call these words with unrelated meanings  *homonyms*. Another term is *polysemy*, which means a word can mean the same thing but have a slightly different meaning. For example,\n",
        "\n",
        "*\"Joan wrote a program to calculate eucledian distance\"*\n",
        "\n",
        "*\"The program featured Mozart's The Marriage of Figaro\"*\n",
        "\n",
        "In short, we need a way of finding the meaning of words based on their relevance to other words in the text. Step forward, Attention.\n",
        "\n"
      ],
      "metadata": {
        "id": "iBhfeBcWjJWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention\n",
        "\n",
        "There are two steps in the transformer during which the model learns what words and text mean. This in ML parlance is updating the \"hidden state\" for inputs to the model.\n",
        "\n",
        "The first is the attention stage, the transformer compares each word to all the other words in a sequence, looking for context and shared significance.\n",
        "\n",
        "The second is the feed forward step, where the model tries to capture more complex patterns and relationships between words. These are accomplished by mathematical transformations.\n",
        "\n",
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/attention_diagram.png?raw=true\" width=\"800\"/>\n",
        "\n",
        "[Diagram](https://distill.pub/2016/augmented-rnns/) from Olah and Carter, 2016"
      ],
      "metadata": {
        "id": "vHlu_HywWOoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From text to genomics and vision"
      ],
      "metadata": {
        "id": "2CdY5WvC42gI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/vit_transformer.png?raw=true\" width=\"800\"/>\n",
        "\n",
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/vit_attention.png?raw=true\" width=\"400\"/>\n",
        "\n",
        "Images from [\"An image is worth 16 x 16 words\"](https://arxiv.org/pdf/2010.11929.pdf)"
      ],
      "metadata": {
        "id": "85ockmDa4k91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('ojQB7PYaU28')"
      ],
      "metadata": {
        "id": "NWlLNw9hGiLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-attention\n",
        "\n",
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/self_attention.png?raw=true\" width=\"800\"/>"
      ],
      "metadata": {
        "id": "x593Ys3ZPktQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-attention is the process of learning the relevance of each word to all other words in the sequence, and is an O(n²), quadratic operation.\n",
        "\n",
        "### Query, key, value\n",
        "\n",
        "The *query* vector checks its own characteristics against the every *key* vector and the network calculates a *value* based on how related the keys are to the query."
      ],
      "metadata": {
        "id": "jOMF7n_F1bEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/multi_head.png?raw=true\" width=\"800\"/>"
      ],
      "metadata": {
        "id": "zPgv0WH4Pkv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/rastringer/code_first_ml/blob/main/images/transformer_encoder.png?raw=true\" width=\"800\"/>"
      ],
      "metadata": {
        "id": "LvAuNBn8n9aV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With each layer, the model's understanding of the text improves. Here are the outputs up to layer 23 of GPT-2 when given the following prompt:\n",
        "\n",
        "\"Q: What is the capital of France?\"\n",
        "\"A: Paris\"\n",
        "\"Q: What is the capital of Poland?\"\n",
        "\"A: \"\n",
        "\n",
        "```\n",
        "0  ( [ The:,\n",
        " at and Act A\n",
        "1  A The ( [ Is59\n",
        " At and40\n",
        "2  A [ ( The At Is Act at59,\n",
        "3  A [ ( Act At Is The CH An at\n",
        "4  A [ At Q (Q The Are M An\n",
        "5  A M No At The payable Q Qu (Q\n",
        "6  No M A The C Die An H En Qu\n",
        "7  C A No The M n P N H An\n",
        "8  A The C P H No n Ass N T\n",
        "9  A C No nil The Ch P An H N\n",
        "10  A The G C N P No Me An Le\n",
        "11  A C N None P G The Pr Ce H\n",
        "12  Unknown None C G A N Bar The Ch P\n",
        "13  C P N G B A Unknown St None The\n",
        "14  St N G P Poland B C Pol A D\n",
        "15  Poland P St Pol Warsaw Polish N B G Germany\n",
        "16  Poland Warsaw Polish Poles Budapest Prague Pol Germany Berlin Moscow\n",
        "17  Poland Warsaw Polish Poles Budapest Prague � Pol Lithuania Moscow\n",
        "18  Poland Warsaw Polish Prague Budapest Poles Moscow � Berlin Kiev\n",
        "19  Warsaw Poland Polish Budapest Prague Moscow Berlin Kiev � Frankfurt\n",
        "20  Warsaw Poland Prague Budapest Polish Moscow Kiev Berlin Frankfurt Brussels\n",
        "21  Warsaw Poland Polish Prague Budapest � Kiev Sz Berlin Moscow\n",
        "22  Warsaw Poland Prague Budapest K W Kiev Sz Moscow Berlin\n",
        "23  Warsaw W K Br Po B L Z P Poland\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "ZA2eQ6XcpW-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j3gTAg2IPky9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Calculate projection dimension\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Create linear projections for key, query, and value vectors for\n",
        "        # each head\n",
        "        self.query_proj = nn.Linear(embed_dim, self.head_dim * num_heads)\n",
        "        self.key_proj = nn.Linear(embed_dim, self.head_dim * num_heads)\n",
        "        self.value_proj = nn.Linear(embed_dim, self.head_dim * num_heads)\n",
        "\n",
        "        # Output projection to project multi-head attention concatenated outputs\n",
        "        # back to original embed_dim\n",
        "        self.output_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Softmax attention calculation\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Get input/output batch sizes and sequence length\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "\n",
        "        # Project inputs to queries, keys, values\n",
        "        # Split last dimension into self.num_heads because we want a separate\n",
        "        # head for each projection\n",
        "        # Note: .view allows you to reshape tensors for operations like\n",
        "        # splitting the vector into heads, flattening sequences, etc.\n",
        "        # while keeping the same data.\n",
        "        queries = self.query_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        keys = self.key_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        values = self.value_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose dimensions so dot products calculate attention weights\n",
        "        # over heads effectively\n",
        "        queries = queries.transpose(1, 2)  # [batch_size, num_heads, seq_len, head_dim]\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention scores\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply softmax to normalize attention weights across heads\n",
        "        attention_weights = self.softmax(scores)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        output = torch.matmul(attention_weights, values)\n",
        "\n",
        "        # Concatenate heads into seq_len flattened batch\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "\n",
        "        # Flatten heads and seq_len out to project\n",
        "        # back to original embed_dim\n",
        "        output = output.view(batch_size, seq_len, embed_dim)\n",
        "\n",
        "        # Project multi-head attention concatenated outputs back to embed_dim\n",
        "        output = self.output_proj(output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "HepFpPikQelu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example input\n",
        "x = torch.tensor([[[1,2,3,4], [5,6,7,8], [9,10,11,12]]])\n",
        "\n",
        "embed_dim = 4 # Embedded dimension from input\n",
        "num_heads = 2 # Number of heads\n",
        "\n",
        "# Instantiate module\n",
        "model = SelfAttention(embed_dim, num_heads)\n",
        "\n",
        "# Run forward pass\n",
        "output = model(x)\n",
        "\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "5e9_V7CjHZ6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This SelfAttention module is implementing multi-headed self-attention, which is a key component of transformers.\n",
        "\n",
        "The main idea is that we're going to project the input embeddings into multiple \"heads\", where each head represents a different learned representation subspace.\n",
        "\n",
        "These subspaces could include:\n",
        "\n",
        "* Positional relationships between words eg\n",
        "  * \"They put **the rug** in the middle of the room and the dog went to sleep on **it**\"\n",
        "\n",
        "* Syntactic roles\n",
        "  * identifying part-of-speech or syntactic dependencies eg *verb*, *suject*, *object*\n",
        "\n",
        "* Semantic relationships\n",
        "* Word importance\n",
        "\n",
        "So basically anything that can help a program understand the meaning of a text.\n",
        "\n",
        "\n",
        "The key components:\n",
        "\n",
        "* `embed_dim`: The input embedding dimensionality (e.g. size of each input token vector)\n",
        "* `num_heads`: The number of parallel attention heads\n",
        "* `head_dim`: The dimension of each attention head. This is embed_dim // num_heads so all heads concatenated together equals the input dimension.\n",
        "* `query_proj`: A linear layer that projects the input into queries, one set per head\n",
        "* `key_proj`: Projects inputs into keys, one set per head\n",
        "* `value_proj`: Projects inputs into values, one set per head\n",
        "\n",
        "Then for each head, we compute attention weights between queries and keys with softmax. And apply those weights to the values:\n",
        "\n",
        "* softmax: Normalizes the attention weights per head\n",
        "* output_proj: Projects multi-head attention outputs back to original input dimension"
      ],
      "metadata": {
        "id": "RyPYLTq-vuf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word vectors\n",
        "\n",
        "GPT-3 uses word vectors of 12,288 dimensions, which means that *each word* in an input training text is represented by a vector of 12,288 numbers. This means the model has almost 13,000 bits of scrap paper to make notes about how the words relate to one another. Those scribbled notes are refined over and over by later layers.\n",
        "\n"
      ],
      "metadata": {
        "id": "XouWlzL3xwGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed it forward\n",
        "\n",
        "In the *attention* layers, the transformer model check all words against all other words in a sequence and gathers information about relevance, meaning and relationship.\n",
        "\n",
        "In the feed forward step, the model muses about what it has learned in the attention step, draws more intricate inferences, and tries to predict the next, *masked* word in a sequence.\n",
        "\n",
        "If we were training a language model on the text of Harry Potter and The Philospher's Stone, for example, the advancing layers of the transformer may look something like this (dramatised example):\n",
        "\n",
        "```\n",
        "adsfkl aer wand aklj london ajsfdhkla Harry and the the j magic\n",
        "```\n",
        "\n",
        "layer 15:\n",
        "\n",
        "```\n",
        "Harry train Hogwarts sweets magic wand Ron school\n",
        "```\n",
        "\n",
        "layer 30:\n",
        "\n",
        "```\n",
        "Harry is a wizard. Ron is Harry's friend. Hermoine is good at spells.\n",
        "```\n",
        "\n",
        "layer 300:\n",
        "\n",
        "```\n",
        "Harry is a diligent, talented young wizard who\n",
        "\n",
        "feels destined to take on the forces of evil\n",
        "\n",
        "magic. Ron is his affable sidekick and the two\n",
        "\n",
        "enjoy their adventures together. Hermoine is a\n",
        "\n",
        "sharp voice of reason and a good friend to\n",
        "\n",
        "both. Ron seems both adoring of and intimidated by Hermoine.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "r3iihBp_yPWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The power of the feed forward neural net comes from its huge connections. There are 49,152 neurons GPT-3's hidden layer. For more details, please read this [excellent article](https://www.understandingai.org/p/large-language-models-explained-with) by Timothy B Lee and Sean Trott on [understandingai.org](understandingai.org).\n",
        "\n",
        "This means:\n",
        "* 12,288 inputs (the word vector)\n",
        "* 49,152 weight parameters\n",
        "* Each feed-forward layer has 49,152 * 12,288 + 12,288 * 49,152 = 1.2 billion weight parameters.\n",
        "* With 96 feed forward layers, this equals 116 billion parameters."
      ],
      "metadata": {
        "id": "wRrdnRcB3Vf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Calculate projection dimension\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Create linear projections for key, query, and value vectors for\n",
        "        # each head\n",
        "        self.query_proj = nn.Linear(embed_dim, self.head_dim * num_heads)\n",
        "        self.key_proj = nn.Linear(embed_dim, self.head_dim * num_heads)\n",
        "        self.value_proj = nn.Linear(embed_dim, self.head_dim * num_heads)\n",
        "\n",
        "        # Output projection to project multi-head attention concatenated outputs\n",
        "        # back to original embed_dim\n",
        "        self.output_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Softmax attention calculation\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Get input/output batch sizes and sequence length\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "\n",
        "        # Project inputs to queries, keys, values\n",
        "        # Split last dimension into self.num_heads because we want a separate\n",
        "        # head for each projection\n",
        "        queries = self.query_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        keys = self.key_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        values = self.value_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose dimensions so dot products calculate attention weights\n",
        "        # over heads effectively\n",
        "        queries = queries.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Calculate dot product attention scores\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply softmax to normalize attention weights across heads\n",
        "        attention_weights = self.softmax(scores)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        output = torch.matmul(attention_weights, values)\n",
        "\n",
        "        # Concatenate heads into seq_len flattened batch\n",
        "        output = output.transpose(1, 2).contiguous()\n",
        "\n",
        "        # Flatten heads and seq_len out to project\n",
        "        # back to original embed_dim\n",
        "        output = output.view(batch_size, seq_len, embed_dim)\n",
        "\n",
        "        # Project multi-head attention concatenated outputs back to embed_dim\n",
        "        output = self.output_proj(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "fnSIlcQAIHMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full translation example\n",
        "\n",
        "This following example is from bentrevett's excellent seq2seq [course](https://github.com/bentrevett/pytorch-seq2seq) on GitHub.\n"
      ],
      "metadata": {
        "id": "K1fmbl6TIOtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install spacy torchtext evaluate tqdm"
      ],
      "metadata": {
        "id": "zp3zfpOwIYSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import spacy\n",
        "import datasets\n",
        "import torchtext\n",
        "import tqdm\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "metadata": {
        "id": "WIVxaU6VIH1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1234\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "F3fmBrg0IVMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"bentrevett/multi30k\")"
      ],
      "metadata": {
        "id": "aEenFoh-IWtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, valid_data, test_data = (\n",
        "    dataset[\"train\"],\n",
        "    dataset[\"validation\"],\n",
        "    dataset[\"test\"],\n",
        ")"
      ],
      "metadata": {
        "id": "RmODSoi-IkYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl"
      ],
      "metadata": {
        "id": "1NhTtKSJJGlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download en_core_web_sm de_core_news_sm"
      ],
      "metadata": {
        "id": "6LkS4wAUImDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "de_nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "PwH_1EhLJUNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
        "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
        "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n",
        "    if lower:\n",
        "        en_tokens = [token.lower() for token in en_tokens]\n",
        "        de_tokens = [token.lower() for token in de_tokens]\n",
        "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
        "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
        "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}"
      ],
      "metadata": {
        "id": "ieOAaTPkJCDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 1_000\n",
        "lower = True\n",
        "sos_token = \"<sos>\"\n",
        "eos_token = \"<eos>\"\n",
        "\n",
        "fn_kwargs = {\n",
        "    \"en_nlp\": en_nlp,\n",
        "    \"de_nlp\": de_nlp,\n",
        "    \"max_length\": max_length,\n",
        "    \"lower\": lower,\n",
        "    \"sos_token\": sos_token,\n",
        "    \"eos_token\": eos_token,\n",
        "}\n",
        "\n",
        "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
        "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
        "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
      ],
      "metadata": {
        "id": "X8wAbZVUJLcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_freq = 2\n",
        "unk_token = \"<unk>\"\n",
        "pad_token = \"<pad>\"\n",
        "\n",
        "special_tokens = [\n",
        "    unk_token,\n",
        "    pad_token,\n",
        "    sos_token,\n",
        "    eos_token,\n",
        "]\n",
        "\n",
        "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    train_data[\"en_tokens\"],\n",
        "    min_freq=min_freq,\n",
        "    specials=special_tokens,\n",
        ")\n",
        "\n",
        "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    train_data[\"de_tokens\"],\n",
        "    min_freq=min_freq,\n",
        "    specials=special_tokens,\n",
        ")"
      ],
      "metadata": {
        "id": "nc41OMJAJZUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert en_vocab[unk_token] == de_vocab[unk_token]\n",
        "assert en_vocab[pad_token] == de_vocab[pad_token]\n",
        "\n",
        "unk_index = en_vocab[unk_token]\n",
        "pad_index = en_vocab[pad_token]"
      ],
      "metadata": {
        "id": "6SQizcRAJgDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_vocab.set_default_index(unk_index)\n",
        "de_vocab.set_default_index(unk_index)"
      ],
      "metadata": {
        "id": "HRZXjqVsJi3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numericalize_example(example, en_vocab, de_vocab):\n",
        "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
        "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
        "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}"
      ],
      "metadata": {
        "id": "fDE58s3TJohH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n",
        "\n",
        "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
        "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
        "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
      ],
      "metadata": {
        "id": "AepTsEnwJkkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_type = \"torch\"\n",
        "format_columns = [\"en_ids\", \"de_ids\"]\n",
        "\n",
        "train_data = train_data.with_format(\n",
        "    type=data_type, columns=format_columns, output_all_columns=True\n",
        ")\n",
        "\n",
        "valid_data = valid_data.with_format(\n",
        "    type=data_type,\n",
        "    columns=format_columns,\n",
        "    output_all_columns=True,\n",
        ")\n",
        "\n",
        "test_data = test_data.with_format(\n",
        "    type=data_type,\n",
        "    columns=format_columns,\n",
        "    output_all_columns=True,\n",
        ")"
      ],
      "metadata": {
        "id": "iRjw3w-XJmMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_collate_fn(pad_index):\n",
        "    def collate_fn(batch):\n",
        "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
        "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
        "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
        "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
        "        batch = {\n",
        "            \"en_ids\": batch_en_ids,\n",
        "            \"de_ids\": batch_de_ids,\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "    return collate_fn"
      ],
      "metadata": {
        "id": "fk_nWh7lJsZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
        "    collate_fn = get_collate_fn(pad_index)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=collate_fn,\n",
        "        shuffle=shuffle,\n",
        "    )\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "nvzQXlu8Ju3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate via Colab (if on Vertex Workbench, you're already authenticated)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "B8RucHNDKEUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
        "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
        "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
      ],
      "metadata": {
        "id": "f59yCAvqJwlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src = [src length, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded = [src length, batch size, embedding dim]\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        # outputs = [src length, batch size, hidden dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
        "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        # outputs are always from the last layer\n",
        "        # hidden [-2, :, : ] is the last of the forwards RNN\n",
        "        # hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        # initial decoder hidden is final hidden state of the forwards and backwards\n",
        "        # encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(\n",
        "            self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        )\n",
        "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
        "        # hidden = [batch size, decoder hidden dim]\n",
        "        return outputs, hidden"
      ],
      "metadata": {
        "id": "dvY1h0PPJzDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn_fc = nn.Linear(\n",
        "            (encoder_hidden_dim * 2) + decoder_hidden_dim, decoder_hidden_dim\n",
        "        )\n",
        "        self.v_fc = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden = [batch size, decoder hidden dim]\n",
        "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_length = encoder_outputs.shape[0]\n",
        "        # repeat decoder hidden state src_length times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_length, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # hidden = [batch size, src length, decoder hidden dim]\n",
        "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
        "        energy = torch.tanh(self.attn_fc(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        # energy = [batch size, src length, decoder hidden dim]\n",
        "        attention = self.v_fc(energy).squeeze(2)\n",
        "        # attention = [batch size, src length]\n",
        "        return torch.softmax(attention, dim=1)"
      ],
      "metadata": {
        "id": "OOyAP1gvJ0_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_dim,\n",
        "        embedding_dim,\n",
        "        encoder_hidden_dim,\n",
        "        decoder_hidden_dim,\n",
        "        dropout,\n",
        "        attention,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "        self.rnn = nn.GRU((encoder_hidden_dim * 2) + embedding_dim, decoder_hidden_dim)\n",
        "        self.fc_out = nn.Linear(\n",
        "            (encoder_hidden_dim * 2) + decoder_hidden_dim + embedding_dim, output_dim\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # input = [batch size]\n",
        "        # hidden = [batch size, decoder hidden dim]\n",
        "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
        "        input = input.unsqueeze(0)\n",
        "        # input = [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded = [1, batch size, embedding dim]\n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "        # a = [batch size, src length]\n",
        "        a = a.unsqueeze(1)\n",
        "        # a = [batch size, 1, src length]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        # weighted = [batch size, 1, encoder hidden dim * 2]\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        # weighted = [1, batch size, encoder hidden dim * 2]\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        # rnn_input = [1, batch size, (encoder hidden dim * 2) + embedding dim]\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        # output = [seq length, batch size, decoder hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, decoder hid dim]\n",
        "        # seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        # output = [1, batch size, decoder hidden dim]\n",
        "        # hidden = [1, batch size, decoder hidden dim]\n",
        "        # this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
        "        # prediction = [batch size, output dim]\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "metadata": {
        "id": "_Y6i9rr4KxnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio):\n",
        "        # src = [src length, batch size]\n",
        "        # trg = [trg length, batch size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        batch_size = src.shape[1]\n",
        "        trg_length = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
        "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
        "        # hidden = [batch size, decoder hidden dim]\n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        input = trg[0, :]\n",
        "        for t in range(1, trg_length):\n",
        "            # insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            # receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
        "            # output = [batch size, output dim]\n",
        "            # hidden = [n layers, batch size, decoder hidden dim]\n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            # get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            # input = [batch size]\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "hyS1clekK06O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = len(de_vocab)\n",
        "output_dim = len(en_vocab)\n",
        "encoder_embedding_dim = 256\n",
        "decoder_embedding_dim = 256\n",
        "encoder_hidden_dim = 512\n",
        "decoder_hidden_dim = 512\n",
        "encoder_dropout = 0.5\n",
        "decoder_dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "attention = Attention(encoder_hidden_dim, decoder_hidden_dim)\n",
        "\n",
        "encoder = Encoder(\n",
        "    input_dim,\n",
        "    encoder_embedding_dim,\n",
        "    encoder_hidden_dim,\n",
        "    decoder_hidden_dim,\n",
        "    encoder_dropout,\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    output_dim,\n",
        "    decoder_embedding_dim,\n",
        "    encoder_hidden_dim,\n",
        "    decoder_hidden_dim,\n",
        "    decoder_dropout,\n",
        "    attention,\n",
        ")\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)"
      ],
      "metadata": {
        "id": "lPVMdu0YK2ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if \"weight\" in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "id": "_nM9c-eOQnsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
      ],
      "metadata": {
        "id": "KsuL_mehQpV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
      ],
      "metadata": {
        "id": "BuJmnLrIQrLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(\n",
        "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
        "):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        src = batch[\"de_ids\"].to(device)\n",
        "        trg = batch[\"en_ids\"].to(device)\n",
        "        # src = [src length, batch size]\n",
        "        # trg = [trg length, batch size]\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg, teacher_forcing_ratio)\n",
        "        # output = [trg length, batch size, trg vocab size]\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
        "        trg = trg[1:].view(-1)\n",
        "        # trg = [(trg length - 1) * batch size]\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "8bfgDSSoQser"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_fn(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            src = batch[\"de_ids\"].to(device)\n",
        "            trg = batch[\"en_ids\"].to(device)\n",
        "            # src = [src length, batch size]\n",
        "            # trg = [trg length, batch size]\n",
        "            output = model(src, trg, 0)  # turn off teacher forcing\n",
        "            # output = [trg length, batch size, trg vocab size]\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
        "            trg = trg[1:].view(-1)\n",
        "            # trg = [(trg length - 1) * batch size]\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "BYBKQkMDQtnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 3\n",
        "clip = 1.0\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "best_valid_loss = float(\"inf\")\n",
        "\n",
        "for epoch in tqdm.tqdm(range(n_epochs)):\n",
        "    train_loss = train_fn(\n",
        "        model,\n",
        "        train_data_loader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        clip,\n",
        "        teacher_forcing_ratio,\n",
        "        device,\n",
        "    )\n",
        "    valid_loss = evaluate_fn(\n",
        "        model,\n",
        "        valid_data_loader,\n",
        "        criterion,\n",
        "        device,\n",
        "    )\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), \"tut3-model.pt\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
      ],
      "metadata": {
        "id": "v_okKO_IQu_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"tut3-model.pt\"))\n",
        "\n",
        "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
        "\n",
        "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
      ],
      "metadata": {
        "id": "wai6x88LQwkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(\n",
        "    sentence,\n",
        "    model,\n",
        "    en_nlp,\n",
        "    de_nlp,\n",
        "    en_vocab,\n",
        "    de_vocab,\n",
        "    lower,\n",
        "    sos_token,\n",
        "    eos_token,\n",
        "    device,\n",
        "    max_output_length=25,\n",
        "):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if isinstance(sentence, str):\n",
        "            de_tokens = [token.text for token in de_nlp.tokenizer(sentence)]\n",
        "        else:\n",
        "            de_tokens = [token for token in sentence]\n",
        "        if lower:\n",
        "            de_tokens = [token.lower() for token in de_tokens]\n",
        "        de_tokens = [sos_token] + de_tokens + [eos_token]\n",
        "        ids = de_vocab.lookup_indices(de_tokens)\n",
        "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
        "        encoder_outputs, hidden = model.encoder(tensor)\n",
        "        inputs = en_vocab.lookup_indices([sos_token])\n",
        "        attentions = torch.zeros(max_output_length, 1, len(ids))\n",
        "        for i in range(max_output_length):\n",
        "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
        "            output, hidden, attention = model.decoder(\n",
        "                inputs_tensor, hidden, encoder_outputs\n",
        "            )\n",
        "            attentions[i] = attention\n",
        "            predicted_token = output.argmax(-1).item()\n",
        "            inputs.append(predicted_token)\n",
        "            if predicted_token == en_vocab[eos_token]:\n",
        "                break\n",
        "        en_tokens = en_vocab.lookup_tokens(inputs)\n",
        "    return en_tokens, de_tokens, attentions[: len(en_tokens) - 1]"
      ],
      "metadata": {
        "id": "TkGjYjxqQyVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(sentence, translation, attention):\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    attention = attention.squeeze(1).numpy()\n",
        "    cax = ax.matshow(attention, cmap=\"bone\")\n",
        "    ax.set_xticks(ticks=np.arange(len(sentence)), labels=sentence, rotation=90, size=15)\n",
        "    translation = translation[1:]\n",
        "    ax.set_yticks(ticks=np.arange(len(translation)), labels=translation, size=15)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "obsAzhg3Qzij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = test_data[0][\"de\"]\n",
        "expected_translation = test_data[0][\"en\"]\n",
        "\n",
        "sentence, expected_translation"
      ],
      "metadata": {
        "id": "pVoWOlHfQ02A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation, sentence_tokens, attention = translate_sentence(\n",
        "    sentence,\n",
        "    model,\n",
        "    en_nlp,\n",
        "    de_nlp,\n",
        "    en_vocab,\n",
        "    de_vocab,\n",
        "    lower,\n",
        "    sos_token,\n",
        "    eos_token,\n",
        "    device,\n",
        ")\n"
      ],
      "metadata": {
        "id": "mzdOexMxQ1_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation"
      ],
      "metadata": {
        "id": "Qbig7uv6Q3J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens"
      ],
      "metadata": {
        "id": "6kJXS4XxQ4cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_attention(sentence_tokens, translation, attention)\n"
      ],
      "metadata": {
        "id": "yZcuG5L0Q6bF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}